[ { "title": "Processing Large Datasets Efficiently with Spring Boot and JDK", "url": "/blog/posts/processing-large-datasets/", "categories": "spring", "tags": "spring-boot", "date": "2025-03-04 12:52:00 -0600", "snippet": "Handling large datasets efficiently requires careful consideration of three key factors: Input – Where the data originates (CSV, Oracle, etc.). Process – How the data is processed (Spring Boot, m...", "content": "Handling large datasets efficiently requires careful consideration of three key factors: Input – Where the data originates (CSV, Oracle, etc.). Process – How the data is processed (Spring Boot, multi-threading, batch processing). Output – Where the processed data is stored (Oracle, distributed storage, etc.).This post explores strategies for optimizing data processing using Spring Boot, multi-threading, partitioning, andcloud-based solutions.GitHub Repo : Data ProcessorScenario 1: CSV as Input, Oracle as OutputApproach Start with a single partition. If performance is an issue, break the dataset into multiple chunks.Example: Total Records: 1,000 Chunk Size: 100 Partitions: 10 Each partition independently processes and stores data in Oracle. Optimizations Use Streaming APIs (OpenCSV, Apache Commons CSV) instead of loading the entire CSV into memory. If CSV is very large (&gt;GBs), process it via AWS S3 + Lambda + SQS to distribute workloads.Scenario 2: Oracle as Input, Oracle as OutputApproach Start with a single partition. If performance is still an issue: Optimize the SQL query (indexing, table joins, query restructuring). Refactor processing logic (validate DB/API calls, optimize memory usage). If still slow, break the processing into multiple chunks.Partitioning StrategiesOption 1: Partition All Three (Input, Process, Output)Each partition independently: Fetches data Processes data Stores dataExample: Partition 1 -&gt; Fetch, Process, Store Partition 2 -&gt; Fetch, Process, Store Partition 3 -&gt; Fetch, Process, StoreOption 2: Fetch First, Then Partition Processing &amp; Output Fetch data from Oracle once. Break the dataset into chunks. Process and store in Oracle in smaller batches.Example: Input -&gt; Fetch data (30 records) Partition 1 -&gt; Process &amp; Store (10 records) Partition 2 -&gt; Process &amp; Store (10 records) Partition 3 -&gt; Process &amp; Store (10 records)Scaling StrategiesIf a single instance or multi-threading isn’t enough, we scale further:1. Multi-Threading (Within a Single Instance) Use Spring Batch + TaskExecutor for parallel processing. Use CompletableFuture.supplyAsync() to execute independent tasks concurrently.2. Multi-Instance Processing (Scaling Horizontally) Run multiple application instances where each instance processes a separate partition. Use Kubernetes (EKS) or AWS ECS to manage instances. Coordinate work using Kafka, SQS, or Redis.Example: Instance 1: Processes records 1-1000 Instance 2: Processes records 1001-2000 Instance 3: Processes records 2001-30003. Distributed ProcessingFor extreme scalability, distribute workload across multiple nodes using a message-driven architecture. Kafka / SQS / RabbitMQ for queue-based processing Use Apache Spark (AWS EMR) for large-scale batch processing For real-time data, use Apache FlinkKey Considerations for Performance OptimizationInput Optimization JDBC Cursor &amp; JdbcTemplate – Stream large datasets instead of loading them into memory. Use pagination techniques (OFFSET-FETCH, ROWNUM, PARTITION BY) to fetch in batches.Processing Optimization Worker Queues – Offload processing to Kafka, SQS, and RabbitMQ. Multi-Threading – Execute multiple partitions concurrently. Cache frequently used data (Redis) to reduce redundant DB calls.Output Optimization Batch Inserts (JdbcTemplate.batchUpdate()) instead of inserting one record at a time. Oracle Table Partitioning for handling large datasets efficiently. Indexing Strategy – Ensure indexes optimize read/write operations.Cloud-Based Scaling Solutions1. AWS Step Functions + AWS Lambda Use AWS Lambda to process each chunk independently. Step Functions orchestrate the entire workflow (fetch, process, store). Fully serverless, scales automatically.2. AWS Glue Best for ETL tasks on massive datasets. Supports automatic schema inference and integration with S3, Redshift, and Athena.3. Apache Spark on AWS EMR Distributed batch processing for very large datasets (&gt; TBs). Supports parallel execution across multiple nodes.Final ThoughtsStart with simple optimizations, then scale step by step. For small to medium datasets, optimize SQL queries, refactor processing logic, and introduce multi-threading. For large-scale processing, partition data, introduce worker queues, and scale horizontally. For cloud-native solutions, AWS Lambda, Step Functions, Kafka, and Apache Spark provide scalable alternatives.By choosing the right strategy, you can efficiently process massive datasets while keeping performance and scalabilityin check!" }, { "title": "Table-Level Lock in Oracle", "url": "/blog/posts/table-level-lock-in-oracle/", "categories": "database", "tags": "oracle", "date": "2022-05-10 02:21:00 -0500", "snippet": "Lock modeThe numeric values for this column map to these text values for the lock modes for table locks: 0- NONE: lock requested but not yet obtained 1- NULL 2- ROWS_S (RS): Row Share Lock ...", "content": "Lock modeThe numeric values for this column map to these text values for the lock modes for table locks: 0- NONE: lock requested but not yet obtained 1- NULL 2- ROWS_S (RS): Row Share Lock indicates that the transaction holding the lock on the table has locked rows in the table and intends to updatethem. 3- ROW_X (RX): Row Exclusive Table Lock indicates that the transaction holding the lock has updated table rows or issuedSELECT ... FOR UPDATE. An RXlock allows other transactions to query, insert, update, delete, or lock rows concurrently in the same table. 4- SHARE (S): Share Table Lock A share table lock held by a transaction allows other transactions to query the table (without usingSELECT ... FOR UPDATE), but updates are allowed only if a single transaction holds the share table lock. 5- S/ROW-X (SRX): Share Row Exclusive Table Lock Only one transaction at a time can acquire an SRX lock on a given table. An SRX lock held by a transactionallows other transactions to query the table (except forSELECT ...FOR UPDATE) but not to update the table. 6- Exclusive (X): Exclusive Table Lock This lock is the most restrictive, prohibiting other transactions from performing any type of DML statement orplacing any type of lock on the table. Last Executed Queryselect *from v$sqlwhere service = 'ee.oracle.docker' and module = 'JDBC Thin Client'order by LAST_ACTIVE_TIME desc;Lock acquired by Tableselect a.locked_mode, c.owner, c.object_name, c.object_type, b.sid, b.serial#, b.status, b.osuser, b.machinefrom v$locked_object a, v$session b, dba_objects cwhere b.sid = a.session_id and a.object_id = c.object_id;Active Session and Processselect s.sid, s.serial#, spid os_pid, s.event, s.username, s.program, s.sql_id, s.logon_timefrom v$session s, v$process pwhere s.paddr = p.addr and s.username = 'SYSTEM'order by logon_time;Check Lock Holder/WaiterSELECT DECODE(request, 0, 'Holder: ', 'Waiter: ') || sid \"Session ID\", id1, id2, lmode, request, typeFROM V$LOCKWHERE (id1, id2, type) IN (SELECT id1, id2, type FROM V$LOCK WHERE request &gt; 0)ORDER BY id1, request;Blocking Objects and SessionsSELECT B.HOLDING_SESSION || ':' || S1.SERIAL# HOLDING_SID_SRN#, S1.USERNAME HOLDER, W.LOCK_TYPE, W.MODE_HELD, S1.MACHINE, S1.MODULE, W.WAITING_SESSION || ':' || S2.SERIAL# WAITING_SID_SRN#, S2.USERNAME WAITER, L.OBJECT_ID, O.OBJECT_NAMEFROM DBA_BLOCKERS B, DBA_WAITERS W, V$LOCKED_OBJECT L, DBA_OBJECTS O, v$session S1, v$session S2WHERE B.HOLDING_SESSION = W.HOLDING_SESSION AND B.HOLDING_SESSION = L.SESSION_ID AND B.HOLDING_SESSION = S1.SID AND W.WAITING_SESSION = S2.SID AND L.OBJECT_ID = O.OBJECT_ID;Check Session Blocking Sessionselect s1.inst_id || ':' || s1.sid || ':' || s1.serial# BLOCKING_INST_SESS_SER#, ' IS BLOCKING ' ACTION, s2.inst_id || ':' || s2.sid || ':' || s2.serial# BLOCKING_INST_SESS_SER#, round(s1.last_call_et / 60) FOR_MINUTESfrom gv$lock l1, gv$lock l2, gv$session s1, gv$session s2where l1.block &gt; 0 and l2.request &gt; 0 and l1.id1 = l2.id1 and l1.id2 = l2.id2 and l1.sid = s1.sid and l1.inst_id = s1.inst_id and l2.sid = s2.sid and l2.inst_id = s2.inst_idorder by l1.inst_id;Total Blocking Session Timeselect nvl(max(round(s1.last_call_et / 60)), 0) FOR_MINUTESfrom gv$lock l1, gv$lock l2, gv$session s1, gv$session s2where l1.block &gt; 0 and l2.request &gt; 0 and l1.id1 = l2.id1 and l1.id2 = l2.id2 and l1.sid = s1.sid and l1.inst_id = s1.inst_id and l2.sid = s2.sid and l2.inst_id = s2.inst_idorder by l1.inst_id;References # V$LOCKED_OBJECT Data Concurrency and Consistency" }, { "title": "Table-Level Lock in PostgreSQL", "url": "/blog/posts/table-level-lock-in-postgres/", "categories": "database", "tags": "postgres", "date": "2022-05-10 01:49:00 -0500", "snippet": "Tables that provide Stats pg_stat_activity : shows information related to the current activity of that process. pg_class : catalogs tables that shows information about table, view, index, etc. p...", "content": "Tables that provide Stats pg_stat_activity : shows information related to the current activity of that process. pg_class : catalogs tables that shows information about table, view, index, etc. pg_locks : provides information about the locks held by active processes. pg_stat_all_tables : shows statistics about accesses of the table. pg_stat_all_indexes : shows statistics about accesses of the index.Table-Level Lock ModesThere are many table-level lock modes but these are the important ones for data query and update. ACCESS SHARE : TheSELECTcommand acquires a lock of this mode on referenced tables. ROW SHARE : TheSELECT FOR UPDATEandSELECT FOR SHAREcommands acquire a lock of this mode on the target table(s). ROW EXCLUSIVE : The commandsUPDATE,DELETE, andINSERTacquire this lock mode on the target table.Identify Table-Level Lockselect relname as relation_name, mode, query, pg_locks.*from pg_locks join pg_class on pg_locks.relation = pg_class.oid join pg_stat_activity psa on pg_locks.pid = psa.pidwhere relname not like 'pg_%';Identify Blocking Lockselect activity.pid, activity.usename, activity.query, blocking.pid as blocking_id, blocking.query as blocking_queryfrom pg_stat_activity as activity join pg_stat_activity as blocking on blocking.pid = any (pg_blocking_pids(activity.pid));Queries to identify Sequence and Index Scanselect relname, seq_scan, idx_scan, autovacuum_count, last_autovacuum, autoanalyze_count, last_autoanalyzefrom pg_stat_all_tableswhere relname not like 'pg_%';select relname, indexrelname, idx_scanfrom pg_stat_all_indexeswhere relname not like 'pg_%';Reference Lock Mode Types" }, { "title": "Using Index in PostgreSQL and Oracle", "url": "/blog/posts/using-index-in-postgres-oracle/", "categories": "database", "tags": "postgres, oracle", "date": "2022-05-09 04:41:00 -0500", "snippet": "Using an index is one of the common ways to optimize performance. And there are things, need to be considered whilecreating it. Like, what happens when multiple fields need to be used for indexing...", "content": "Using an index is one of the common ways to optimize performance. And there are things, need to be considered whilecreating it. Like, what happens when multiple fields need to be used for indexing? what will be the order for those fields? do multiple indexes going to overlap each other? what is the limit for creating an index for any table?These are the few concerns, that will guide you in creating a better index. But in this article, we will only touch thesurface level and know how to start using it.So, let’s create an index and see, how to verify its usage.Run PostgreSQL and Oracleversion: '3'services: postgres: container_name: postgres image: postgres ports: - \"5432:5432\" environment: - POSTGRES_DB=dummy - POSTGRES_PASSWORD=postgres restart: always oracle: container_name: oracle image: ashimjk/oracle-ee-12c ports: - \"1521:1521\"PostgreSQLurl = jdbc:postgresql://localhost:5432/dummyusername = postgrespassword = postgresOracleurl = jdbc:oracle:thin:@localhost:1521/ee.oracle.dockerusername = systempassword = oracleCreate Tablecreate table dummy( id int not null primary key, reference varchar(255));Create Indexcreate index dummy_reference_idx ON dummy (reference);Dummy Datainsert into dummy values (1, 'ref1');insert into dummy values (2, 'ref2');insert into dummy values (3, 'ref3');insert into dummy values (4, 'ref4');insert into dummy values (5, 'ref5');insert into dummy values (6, 'ref6');insert into dummy values (7, 'ref7');insert into dummy values (8, 'ref8');insert into dummy values (9, 'ref9');insert into dummy values (10, 'ref10');Check index usage PostgreSQL Use explain to verify the usage of the indexexplain select * from dummy where id = 1;-- OUTPUTIndex Scan using dummy_pkey on dummy (cost=0.14..8.16 rows=1 width=520) Index Cond: (id = 1)explain select * from dummy where reference = 'ref1';-- OUTPUTIndex Scan using dummy_reference_idx on dummy (cost=0.14..8.16 rows=1 width=520) Index Cond: ((reference)::text = 'ref1'::text) Use explain analyze for more detail.explain analyse select * from dummy where id = 1;-- OUTPUTIndex Scan using dummy_pkey on dummy (cost=0.14..8.16 rows=1 width=520) (actual time=0.034..0.036 rows=1 loops=1) Index Cond: (id = 1)Planning Time: 0.087 msExecution Time: 0.058 msexplain analyse select * from dummy where reference = 'ref1';-- OUTPUTIndex Scan using dummy_reference_idx on dummy (cost=0.14..8.16 rows=1 width=520) (actual time=0.018..0.020 rows=1 loops=1) Index Cond: ((reference)::text = 'ref1'::text)Planning Time: 0.077 msExecution Time: 0.039 msCheck index usage OracleAfter executing the query, dbms_xplan.display table provides a summary of how the query was executed and which indexwas used.explain plan for select * from dummy where id = 1;select * from table(dbms_xplan.display);-- OUTPUT| Id | Operation | Name |----------------------------------------------------| 0 | SELECT STATEMENT | || 1 | TABLE ACCESS BY INDEX ROWID| DUMMY ||* 2 | INDEX UNIQUE SCAN | SYS_C0018370 |explain plan for select * from dummy where reference = 'ref1';select * from table(dbms_xplan.display);-- OUTPUT| Id | Operation | Name |-------------------------------------------------------------------| 0 | SELECT STATEMENT | || 1 | TABLE ACCESS BY INDEX ROWID BATCHED| DUMMY ||* 2 | INDEX RANGE SCAN | DUMMY_REFERENCE_IDX |An alternative way to extract the same summary can be configured in various ways. But you need to use sqlplus or anyother similar tools:set autotrace on explainselect * from dummy where reference = 'ref1';set autotrace offReference Set autotrace in sqlplus Index Usage and Index Performance Test in Oracle Database" }, { "title": "Spring Boot - @TestConfiguration", "url": "/blog/posts/spring-boot-test-config-annotation/", "categories": "spring", "tags": "spring-boot", "date": "2021-12-27 06:31:00 -0600", "snippet": "The @TestConfiguration is one of the annotations, which can be used for writing unit tests or integration tests in aSpring Boot Application.Since, @TestConfiguration is not a new thing, and it has ...", "content": "The @TestConfiguration is one of the annotations, which can be used for writing unit tests or integration tests in aSpring Boot Application.Since, @TestConfiguration is not a new thing, and it has already been explained by many posts/articles or you canrefer to one of them from the reference section below. And, I am not going to add another one to the list.So, what is this article about?I was curious about, how @TestConfiguration works or the scenario for it. I had gone through various posts/articlesbut didn’t find more details about them. So, I tried to create a scenario to understand it better, which lead me towrite this post.This post is all about the experience that I have gathered from the other posts/articles and the scenario that I havecreated for it. Let’s get started…ScenarioIn the test suite, I wanted to provide and as well as override the bean which should be applicable for both theintegration and unit tests. To achieve this behavior, I can use @Configuration annotation with some additionalproperty (this will be explained later).Since Spring Boot provides @TestConfiguration annotation, there has to be some purpose right. Otherwise, why wouldthey create it in the first place? So, this is the journey that we are about to explore.Exploration - Part OneFrom Spring Boot 2.1, the bean overriding feature has been disabled by default, which makes sense for the mainapplication. And if we do that, it will throw an exception called BeanDefinitionOverrideException.We can enable this feature by using the spring.main.allow-bean-definition-overriding property (this is what I wasreferring to earlier). We can add it to the resource file or use the property source to enable it. In our case, it willbe a property source. Note: We should not enable this feature in the main application. If we need to, then it means that our mainapplication is tightly coupled with the beans. But for the test classes, there may be various scenarios where we needtooverride it, like for the datasource, security, infrastructure, etc.Exploration - Part Two@TestConfiguration inherits from the @Configuration annotation. Even though, classes that are annotated with@TestConfiguration are excluded from component scanning. Because of this, we need to import it explicitly in everytest where we want to use it.How and Why is the reason that I am curious about. And this is where things get complicated.First, let’s explore how it is done.In the test, when we define @SpringBootTest annotation, it does a couple of things in the background. Mainly, It uses SpringBootContextLoader as a default if @ContextConfiguration is not defined. It searches for @SpringBootConfiguration or @SpringBootApplication (which inherits @SpringBootConfiguration) ifthe classes property is not specified.In the @SpringBootApplication annotation, it has @ComponentScan annotation where TypeExcludeFilter.class isspecified as an excludeFilters. And subclass of TypeExcludeFilter.class which is TestTypeExcludeFilter.class,matches @TestComponent annotated class.In a nutshell, any classes that are annotated with @TestComponent are excluded from component scanning because ofexcludeFilters present in the @SpringBootApplication annotation. And @TestConfiguration inherits @TestComponentbecause of which it is also excluded.So, this is the answer for how it’s done internally.Now, let’s explore why it is done.If we are creating a bean or overriding it for the test classes, then it means we are doing something different which isnot required by or necessary for the main application. So, it’s better to be excluded from the main application. Also,it gives a hint that the configuration is only for the test classes.This is my answer to the why? There may be a different answer for it.Let’s start the codingWe will create the three beans, namely, DefinedInAppConfig will be defined in the AppConfig DefinedInTestConfig will be defined in TestConfig OverriddenByTestConfig will be defined in both AppConfig and TestConfig.For the test scenario, we will create three classes, namely, DefaultBehaviourTest ImportTestConfigTest OverrideBeanTest NestedTestConfigTestDefaultBehaviourTestIt will fail by saying NoSuchBeanDefinitionException for the DefinedInTestConfig bean which is defined in other testconfigs.ImportTestConfigTestIt will pass because we are importing TestConfig using @Import annotation.OverrideBeanTestIt will pass because we have provided the test config class explicitly in the @SpringBootTest annotation.NestedTestConfigTestHere, we are providing a test configuration as a nested class. It will be picked up by the @SpringBootTest at first.Also, it is another way of overriding a bean.For the code example, please check out the github.References https://www.baeldung.com/spring-boot-testing https://howtodoinjava.com/spring-boot2/testing/springboot-test-configuration/ https://reflectoring.io/spring-boot-testconfiguration/" }, { "title": "Spring Boot - Internals", "url": "/blog/posts/spring-boot-internals/", "categories": "spring", "tags": "spring-boot", "date": "2021-06-25 07:47:00 -0500", "snippet": "In this article, we are going to look at the spring-boot internals, which explainshow it start-up and how does component scanning work. Although, spring boot itself is huge,and many areas might not...", "content": "In this article, we are going to look at the spring-boot internals, which explainshow it start-up and how does component scanning work. Although, spring boot itself is huge,and many areas might not have been explored over here. But I will try to fill all thosegaps as much as possible.Let’s beginIn the main application class, where we define the main method, generally, we add two things: @SpringBootApplication annotation at the class level SpringApplication.run(&lt;MainApplication.class&gt;, args) inside main method@SpringBootApplication annotation is the reason for all the magic that happens inside the spring boot application.And SpringApplication.run method is the trigger for that. @SpringBootApplication is the convenience annotationwhich itself is annotated with: @SpringBootConfiguration: it tells annotated class is the baseline for spring-boot startup @EnableAutoConfiguration: it enables bean creation which is likely to be created, based on dependency found in theclasspath @ComponentScan: it scans the base packages for component and configurationStart-Up of a spring-boot applicationWhen we run the spring-boot application, this is how it flows: First, it lists down all the listeners thatimplement SpringApplicationRunListenerinterface from spring factories. publishes application starting event to all the listeners. prepares the environment which consists of profiles, property sources, property resolver, etc creates the application context based on the application type prepares the application context using environment, listeners, arguments and register the bean factory post-processors refresh the application context its a sequence of steps that the spring container goes through to make it usable invoke bean factory post-processors which registers the bean in the context ConfigurationClassPostProcessor is the one that does the scanning and registration part initialize the message source creates the webserver if it is a web application calls @PostConstruct annotated methods finalize the context, cleanup, and publish context refresh event run all the application and command-line runnerHow spring-boot does component scanning?Setup Ituses @ComponentScanannotation and its properties to identify eligible candidate components It creates a classpath scanner If useDefaultFiltersproperty is true, then it register three annotation to its filter Component ManagedBean Named It creates a bean name generator It adds include and exclude filters to the scanner It tries to identify the base packages using basePackages and basePackageClasses property If it’s empty then it will use the package of declaring class which has @SpringBootApplicationStart scanning the packagesNow based on the above configuration, it will start scanning the classpath in the base packages which matchesthe include filters. For those candidates, it creates the bean definition and bean name.For more detail, you can look at these classes: ComponentScanBeanDefinitionParser ClassPathBeanDefinitionScanner ClassPathScanningCandidateComponentProviderHow spring-boot initialize auto-configurable bean?During component scanning, with thehelp @EnableAutoConfigurationannotation, whichimports AutoConfigurationImportSelector,it tries to identify all the spring.factories files. Inside it, the scanner looks for the EnableAutoConfigurationkey which provides conditional configuration classes. Those who match conditional property which is evaluated based onclasses available on the classpath,beans, or some configuration, are used to register all the beans in the context.Some important classes @SpringBootApplication SpringApplication -where it all begins AbstractApplicationContext -builds the spring container ServletWebServerApplicationContext -if you running the web application, this will be the subclass for AbstractApplicationContext ConfigurationClassPostProcessor -scans and register the bean in the context" }, { "title": "Docker-Compose - Wait for Dependencies", "url": "/blog/posts/wait-for-dependencies/", "categories": "docker-compose", "tags": "development", "date": "2021-06-23 02:09:00 -0500", "snippet": "Don't Repeat Yourself for sake of others. So, there is nothing new in here, that hasn’tbeen elaborated on by many other authors. I am just noting it down and extracting majorparts of it.So, let’s w...", "content": "Don't Repeat Yourself for sake of others. So, there is nothing new in here, that hasn’tbeen elaborated on by many other authors. I am just noting it down and extracting majorparts of it.So, let’s wait for the docker dependencies…If we run docker-compose.yml thenwaiting-for-rabbitmq will printWaiting for rabbitmq... until rabbitmq starts. Sometimes it’s expected behavior but not for us.There are many ways to resolve it and we will look into this one by one.Approach 1: Run the dependencies firstFirst, we will run the dependency along with its status checker service. Once the status is green,then we will run all the other dependent services.Let’s create a scenario, where we will have a web service that will depend on rabbitmq.Now we will add another service inthe docker-compose.yml file,which is like this.services: rabbitmq: container_name: ajk_rabbitmq image: rabbitmq:3.7-management-alpine ports: - 5672:5672 - 15672:15672 web: container_name: ajk_web image: alpine:3.8 command: [ 'sh', '-c', 'nc -z -v rabbitmq 5672' ] depends_on: - rabbitmq links: - rabbitmq waiting_for_rabbitmq: container_name: ajk_waiting_for_rabbitmq image: alpine:3.8 command: [ 'sh', '-c', 'until nc -z -v rabbitmq 5672; do echo \"Waiting for rabbitmq...\" &amp;&amp; sleep 1; done;' ] depends_on: - rabbitmq links: - rabbitmqHow to run Run docker-compose up waiting_for_rabbitmq command It will start rabbitmq service waiting_for_rabbitmq service will wait for rabbitmq to be started Once, it is started, it will print 5672 (IP:5672) open in the console Now, run docker-compose up webAlthough it’s a sequential approach it will do the work for us.Reference: Article GitHubApproach 2: Using health checksWhen werun docker-compose.yml file,first it will run rabbitmq service,and will wait for its status to be healthy.Once, it’s ready, web service will be started and will print the following in the console:ajk_web | rabbitmq (172.22.0.2:5672) openajk_web exited with code 0services: rabbitmq: container_name: ajk_rabbitmq image: rabbitmq:3.7-management-alpine ports: - 5672:5672 - 15672:15672 healthcheck: test: [\"CMD\", \"nc\", \"-z\", \"localhost\", \"5672\"] interval: 10s timeout: 10s retries: 3 web: container_name: ajk_web image: alpine:3.8 command: [\"sh\", \"-c\", \"nc -z -v rabbitmq 5672\"] depends_on: rabbitmq: condition: service_healthy links: - rabbitmqHere, we are using healthcheck configuration option, added in rabbitmq service. web service is depending uponrabbitmq with the condition service_healthy.Note: docker-compose version 3 doesn’t support the condition form of depends_on. We can add restart: on-failurein the web service to get the same behavior. One thing to keep in mind is, now web service will be restarteduntil rabbitmq status is healthy.Reference: Health Checks Depends-On V2 Depends-On V3Other ways to do it wait-for-it dockerize goss Health Check using Goss Start Up Order docker-compose-wait DOCKER COMPOSE WAIT FOR DEPENDENCIES" }, { "title": "Part 9 - Producer/Consumer using Spring Kafka", "url": "/blog/posts/spring-kafka/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "Dependency&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&...", "content": "Dependency&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;Configuring KafkaAdminWe need the KafkaAdmin bean for creating a new topic. It just delegates to an AdminClient for creating and describing the topic.It looks for NewTopic bean in the application context during its initialization and wraps it insideNewTopics list.@Beanpublic KafkaAdmin kafkaAdmin() { Map&lt;String, Object&gt; configs = new HashMap&lt;&gt;(); configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddresses); return new KafkaAdmin(configs);}@Beanpublic NewTopic myTopic() { return new NewTopic(\"myTopic\", 1, (short) 1);}Configuring ProducerFactory and KafkaTemplateWe need the ProducerFactory bean for creating a producer. It is a factory class for creating Producerinstances. And default implementation is provided by DefaultKafkaProducerFactory.We will also need the KafkaTemplate bean, for sending the messages. It provides a high-level operationslike, send the messages which are thread-safe, when used with DefaultKafkaProducerFactory.@Beanpublic ProducerFactory&lt;String, String&gt; producerFactory() { Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;(); configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return new DefaultKafkaProducerFactory&lt;&gt;(configProps);}@Beanpublic KafkaTemplate&lt;String, String&gt; kafkaTemplate() { return new KafkaTemplate&lt;&gt;(producerFactory());}Configuring ConsumerFactory and KafkaListenerContainerFactoryWe need the ConsumerFactory bean for creating a consumer. It is a factory class for creating Consumerinstances. And default implementation is provided by DefaultKafkaConsumerFactory.ConcurrentKafkaListenerContainerFactory is the subclass of KafkaListenerContainerFactory which looks for@KafkaListener annotated methods and are added to the KafkaListenerEndpointRegistry.When messages are received from Kafka Broker, @KafkaListener annotated methods are triggered.@Beanpublic ConsumerFactory&lt;String, String&gt; consumerFactory() { Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); return new DefaultKafkaConsumerFactory&lt;&gt;(props);}@Beanpublic ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; kafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(consumerFactory()); return factory;}Note: @EnableKafka is required in the configuration class which enables detection of KafkaListener annotationson any spring-managed bean in the container.How to runI have implemented spring kafka using two different approaches: By manually configuring the required bean for kafka Using auto-configuration provided by spring-kafkaAlthough both of them are managed using the spring profile: [auto, manual], but does the same thing.For implementation, please visit the github.For Spring Cloud Stream with Kafka implementation, please visit the github here.Reference Spring for Apache Kafka DocsKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client" }, { "title": "Kafka Series", "url": "/blog/posts/kafka/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "StartupFirst, you need to download Apache Kafka.And extract it to your desired folder.If you go inside the Kafka directory, something like this kafka_2.13-2.8.0,you will more likely see config, bin...", "content": "StartupFirst, you need to download Apache Kafka.And extract it to your desired folder.If you go inside the Kafka directory, something like this kafka_2.13-2.8.0,you will more likely see config, bin, lib, etc directories.But for this series, we will only focus on the config and bin directory.ConfigurationThese are the two configuration files that reside inside the config directory,that we will use to start the Zookeeper and the Kafka. zookeeper.properties server.propertiesKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 3 - Run a single instance of Apache Kafka", "url": "/blog/posts/kafka-single-instance/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "To follow the below sections, you need to set up the Apache Kafka.And for that, you can follow the steps define in startup section.After the installation, go to the Kafka directory, something like ...", "content": "To follow the below sections, you need to set up the Apache Kafka.And for that, you can follow the steps define in startup section.After the installation, go to the Kafka directory, something like this kafka_2.13-2.8.0and follow through the steps to create a topic, start producing and consuming the messages.Kafka TopicCreatebin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic my_topicListbin/kafka-topics.sh --list --zookeeper localhost:2181Viewbin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my_topicKafka Console Producerbin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my_topicKafka Console Consumerbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my_topic --from-beginningKafka Message Log Stored in /tmp/kafka-logs For the above topic, my_topic-0 directory will be created View log using cat /tmp/kafka-logs/my_topic-0/00000000000000000000.logKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 5 - Kafka Producer", "url": "/blog/posts/kafka-producer/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "This section describes the Kafka Producer and its internals.For implementation, please visit the github.Three Major Required Properties bootstrap.servers required for a producer to start ...", "content": "This section describes the Kafka Producer and its internals.For implementation, please visit the github.Three Major Required Properties bootstrap.servers required for a producer to start up used for discovering full membership of the cluster used for determining the partition leaders or owners key.serializer value.serializerHere is the full list of producer configs.Producer Record topic Topic to which the ProducerRecord will be sent value The message content (matching the serializer type for value) partition specific partition within the topic to send ProducerRecord timestamp Unix timestamp applied to the recordKafka Producer instances can only send ProducerRecord that matches the key and value serializer types it is configured with. key a value to be used as the basis of determining the partitioning strategy to be employed by the Kafka Producer Additional information in the message Can determine what partitions the message will be written to Record Accumulator Implements Micro-batching pattern Small and fast batches of messages Sending (Producer) Writing (Broker) Reading (Consumer) Uses Queue for storing records Maintains RecordBatch for each topic partitionMessage Buffering Properties batch.size (default: 16384) : buffer size per RecordBatch in bytes buffer.memory (default: 33554432) : buffer size of all record batch in bytes max.block.ms (default: 60000 - 1min) : time for blocking the send record call linger.ms (default: 0) : time to wait before sending recordBroker Acknowledgement - “acks” 0: fire and forget 1: leader acknowledged 2: replication quorum acknowledgedConfiguration for Broker that responds with an error retries (default: 2147483647): how many times producer should try to send the record retry.backoff.ms (default: 100): how many milliseconds does the producer need to wait for retriesMessage Order No guarantee of order if multiple partitions are used If there is an error while sending a record then there may be a chance of an unordered record Set max.in.flight.request.per.connection (default: 5) to 1 if an order needs to be maintained Delivery Semantics at-most-once at-least-once only-onceKafka Producer Internals Properties ~&gt; ProducerConfig Message ~&gt; ProducerRecord Processing Pipeline ~&gt; Serializers and Partitioners Micro-batching ~&gt; RecordAccumulator and RecordBufferSome useful configuration delivery.timeout.ms (default: 120000): an upper bound on the time to report success or failure after a call to send max.request.size (default: 1048576): maximum size of a request in bytesKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 2 - Kafka Partitions", "url": "/blog/posts/kafka-partitions/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "This section shows how partitions are managed for topics, producer,consumer and with replication.Create Kafka TopicKafka Partitions ProducerKafka Partitions ConsumerKafka Partitions with Replicatio...", "content": "This section shows how partitions are managed for topics, producer,consumer and with replication.Create Kafka TopicKafka Partitions ProducerKafka Partitions ConsumerKafka Partitions with ReplicationKafka Series Part 1 - Kafka Key Terminology Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 4 - Run a multiple instances of Apache Kafka", "url": "/blog/posts/kafka-multiple-instance/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "To follow the below sections, you need to set up the Apache Kafka.And for that, you can follow the steps define in startup section.After the installation, go to the Kafka directory, something like ...", "content": "To follow the below sections, you need to set up the Apache Kafka.And for that, you can follow the steps define in startup section.After the installation, go to the Kafka directory, something like this kafka_2.13-2.8.0and follow through the steps to create a topic, start producing and consuming the messages.Zookeeperbin/zookeeper-server-start.sh config/zookeeper.propertiesCheck status Add 4lw.commands.whitelist=* in zookeeper.properties to whitelist command like stat, conf Check the status of zookeeper using telnet localhost 2181Create Configuration for Kafka Instance Make a copy of existing server.properties files to For instance 0 - server-0.properties For instance 1 - server-1.properties For instance 2 - server-2.properties Update broker.id For instance 0 - broker.id=0 For instance 1 - broker.id=1 For instance 2 - broker.id=2 Add server listeners For instance 0 - listeners=PLAINTEXT://localhost:9090 For instance 1 - listeners=PLAINTEXT://localhost:9091 For instance 2 - listeners=PLAINTEXT://localhost:9092 Update log.dirs For instance 0 - log.dirs=/tmp/kafka-logs-0 For instance 1 - log.dirs=/tmp/kafka-logs-1 For instance 2 - log.dirs=/tmp/kafka-logs-2 You can also find these configurations in samples directory.Start Multiple InstanceStart Instance 0bin/kafka-server-start.sh config/server-0.propertiesStart Instance 1bin/kafka-server-start.sh config/server-1.propertiesStart Instance 2bin/kafka-server-start.sh config/server-2.propertiesCheck Broker Registration using Zookeeper telnet localhost 2181 type statCreate Topicbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic replica_topicDescribe Topicbin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic replica_topicOutput:Topic: replica_topic TopicId: 6pL_IeaHRzW_OA4X_in1Og PartitionCount: 1 ReplicationFactor: 3 Configs: Topic: replica_topic Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2Kafka Console Producerbin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic replica_topicKafka Console Consumerbin/kafka-console-consumer.sh --bootstrap-server localhost:9090 --topic replica_topic --from-beginningKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 1 - Kafka Key Terminology", "url": "/blog/posts/kafka-key-terms/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "This section focuses on the key terminology and concepts about Apache Kafka.And also note that each has its contribution to Apache Kafka Architecture.Distributed Messaging SystemKafka is a distribu...", "content": "This section focuses on the key terminology and concepts about Apache Kafka.And also note that each has its contribution to Apache Kafka Architecture.Distributed Messaging SystemKafka is a distributed messaging system which is designed for: high throughput reliability resilient fast movement of data through the more and more diverse system, particularly when data is growingProducers / PublisherA publisher creates some data and sends it to a broker. Producers are simply an applicationsthat you write to send the messages.Consumers / SubscriberA subscriber retrieves the message and processes it from the broker to whichit is interested and authorized. Consumers are simply applicationsthat you write to receive the messages.TopicsIt is the collection of messages or grouping of messages. And every topic has its name, which is used by producers and consumers to send/receive messages. Topics canbe defined upfront or on-demand as needed.Broker or Worker or NodeKafka broker is a software process also referred to as executable or daemon servicethat runs on a machine that can be physical or virtual.ClusterA kafka cluster is the grouping of multiple kafka brokers which can be on a single machine or different machines.ControllerA controller is just a worker node or broker like any other. It just happened to be elected from amongstits peer to officiate in the administrative capacity of a controller. Below are the critical responsibilities ofthe controller: To maintain an inventory of what workers are available to take on work. To maintain a list of work items that have been assigned to workers. To maintain the status of workers To manage the replication factor and task redundancy in case of failureLeaderA leader is also a worker node or broker like any other. It is elected by the controller for assigning the task.It will peer with other worker nodes based on the replication factor to complete its task. Once a peer has committed to itsleader, the quorum is formed. And these peer takes all the task that is assigned to its leader.Communication and ConsensusVirtually every component within a cluster has to keep some form of communication going between themselves.In a distributed system, these communications are called consensus. Besides the obvious data payloads being transferredas messages, there are other types of network communication happening that keep the cluster operating normally.These are some of them: Worker node membership and naming Configuration management - startup-config Leader election Health statusZookeeperIn the context of Apache Kafka, ZooKeeper serves as a centralized service for maintaining metadata about a clusterof distributed nodes, which are: Configuration information Health and Synchronization Status Cluster and Quorum Group membership Roles of elected nodesAlso, it is the responsibility of the Zookeeper to assign a broker to be responsible for the topic.Topic message orderEvery message that comes to the broker is grouped by topics and is stored inside a partition.And each partition stores these messages in an ordered sequence (by time) and immutable by nature meanscannot be updated once it is received by the broker. But in the case of multiple partitions, an order cannot beguaranteed and it’s the responsibility of the consumer to handle this use case.Event SouringAn architectural style or approach to maintaining an application’s state by capturing all changes as aa sequence of time-ordered, immutable events.Message ContentEvery message that is consumed by the broker contains these three properties: Timestamp Reference-able identifier Payload (binary)OffsetIt is the last read message position that is maintained by the Kafka consumer. It also corresponds tothe message identifier.Message Retention PolicyIt is the period for how long messages will be stored in the broker. By default, it is set to 168 hours or seven days.The retention period is defined on a per topic basis. Physical storage can also constrain the message retention policy.Distributed Commit Log (Transaction or Commit Logs)It is the series of commit logs that can also be used for event sourcing. Following are the key properties of it: Source of truth Physically stored and maintained Higher-order data structures derived from the log Tables, indexes, views, etc Point of recovery Basis for replication and distributionKafka PartitionsIt is the physical representation of the topic as a commit log stored on one or more brokers. It can be found inside/tmp/kafka-logs/{topic}-{partition} directory, as per our example, /tmp/kafka-logs/my_topic-0 directory will becreated. This directory will contain an index, log files, etc. These are the main properties of partitions: Partitions == Log Each topic has one or more log files called partitions Basis for Scaling Fault-tolerant Higher levels of throughput At-least one partition is required for one or more brokerReplication FactorIt is the factor that decides how much data redundancy should happen inside the cluster.And this is configured on a per-topic basis. These are the features that it provides: Reliable work distribution Redundancy of messages Cluster resiliency Fault-tolerance Guarantees N-1 broker failure tolerance 2 or 3 minimum Broadcast In-Sync Replicas or ISR to the cluster is equal to the replication factor for that topic and partitionNotes The scalability of Apache Kafka is determined by the number of partitions being managed by multiple broker nodesKafka Series Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 6 - Kafka Consumer", "url": "/blog/posts/kafka-consumer/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "This section describes the Kafka Consumer and its internals.For implementation, please visit the github.Three Major Required Properties bootstrap.servers required for the consumer to star...", "content": "This section describes the Kafka Consumer and its internals.For implementation, please visit the github.Three Major Required Properties bootstrap.servers required for the consumer to start up used for discovering full membership of the cluster used for determining the partition leaders or owners key.deserializer value.deserializerHere is the full list of consumer configs.What subscription does? It will use automatic/dynamic partition assignment for topics Providing a single topic means pulling from every partition with that topic (one-to-many relationship) Providing multiple topics means pulling from every partition for all topics (many-to-many relationship)Consumer Coordinator It’s aware of automatic or dynamic partition reassignment Push notification of assignment changes to the subscription state object It commits the offsets to the clusterOffset The message that has been read, does not mean its also committed Configuration for Offset Gap enable.auto.commit = true (default) auto.commit.interval.ms = 5000 (default) auto.offset.reset = “latest” (default) “earliest” “none” Offset behavior varies between whether you are a single consumer or a consumer group topologyOffset Management There are two different ways to manage the offset commit: automatic/manual To use manual offset commit, we need to set enable.auto.commit config to false There are two ways to commit offset in manual mode: commitSync It is a synchronous request which blocks until it receives the response from the cluster In case of error, we can use retry.backoff.ms (default: 100) config for retries until it succeeds or unrecoverable error commitAsync As the name suggests, it is an asynchronous request which a non-blocking but also a non-deterministic It doesn’t have retries functionality It also supports callback option which can help to process even further when the response comes from the cluster Consumer Group It’s a Kafka solution to consumer side scale-out group.id config can be used for defining the consumer group The main goal of the consumer group is to share the message consumption and processing load provides parallelism and high throughput increases the level of redundancy increases the performance of message processing Group Coordinator It is a broker who is elected to serve as a group coordinator It monitors and maintains the consumer groups membership using heartbeat.interval.ms : 3000 (default) session.timeout.ms : 10000 (default) It evenly balances available consumers to partitions It tries to assign one consumer to one partition of that topic but it varies on no of consumer and partitions It also initiates re-balancing protocol which will happen only if new partitions are added any consumer failure Kafka Consumer Internals Properties ~&gt; ConsumerConfig Message ~&gt; ConsumerRecordSome useful configuration fetch.min.bytes (default: 1) : minimum number of bytes that must be returned from the poll fetch.max.bytes (default: 52428800 - 50MB) : maximum number of bytes that must be returned from the poll max.fetch.wait.ms (default: 500) : amount of time to wait if there isn’t enough data to meet above threshold max.partition.fetch.bytes (default: 1048576 - 1MB) : maximum number of bytes to be returned per partition per cycle max.poll.records (default: 500) : maximum number of records allowed per poll cycle max.poll.interval.ms (default: 300000 - 5min) : maximum delay between invocations of poll allow.auto.create.topics (default: true) : allows automatic topic creationSome useful consumer control Consumer Position Control seek() ~&gt; read any specific message by providing an offset seekToBeginning() ~&gt; read message from the beginning seekToEnd() ~&gt; read message from the end Flow Control pause() ~&gt; pause the message for any partition of topic resume() ~&gt; resume the message for any partition of topic Re-balance Listeners ~&gt; listen for consumer group re-balancingKafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 7 - Kafka Challenges Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 8 - Producer/Consumer using Kafka Client", "url": "/blog/posts/kafka-client/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "In this section, we are going to use kafka client api and see how to programmatically create, configure and use Kafka Cluster.Following are the main topic, that we are going to look at: Use Admin ...", "content": "In this section, we are going to use kafka client api and see how to programmatically create, configure and use Kafka Cluster.Following are the main topic, that we are going to look at: Use Admin API to create topics Send the message using Producer Receive the message using ConsumerDependency for Kafka Client&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt;&lt;/dependency&gt;Create ConfigurationKafka requires configuration in key/values pairs. And for that we can use java.util.Properitesclass. Following are required config for Kafka Producer:Properties properties = new Properties();properties.put(\"bootstrap.servers\", \"localhost:9090,localhost:9091\");properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Run Kafka InstanceAlthough, there many ways to start the kafka instance, which are: Using kafka single instance Using kafka multiple instances Using docker-compose Using TestContainers for the test caseDependency for TestContainers&lt;dependency&gt; &lt;groupId&gt;org.testcontainers&lt;/groupId&gt; &lt;artifactId&gt;kafka&lt;/artifactId&gt; &lt;version&gt;${test-container.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.testcontainers&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;${test-container.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;Admin API UsageRequired config bootstrap.serversCreate Kafka Topic ProgrammaticallyIt will create the topic with default options. Note that kafkaFuter.get() will block the threaduntil topic creation has been completed or failed.try (Admin admin = Admin.create(properties)) { int numOfPartitions = 1; short replicationFactor = 1; NewTopic newTopic = new NewTopic(topicName, numOfPartitions, replicationFactor); CreateTopicsResult topicsResult = admin.createTopics(Collections.singleton(newTopic)); KafkaFuture&lt;Void&gt; kafkaFuture = topicsResult.values().get(topicName); kafkaFuture.get();}Kafka ProducerRequired config bootstrap.servers key.serializer value.serializerImplementationtry (KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties)) { messages.entrySet() .stream() .map(p -&gt; new ProducerRecord&lt;&gt;(TOPIC_NAME, p.getKey(), p.getValue())) .forEach(producer::send); } Kafka ConsumerRequired config bootstrap.servers key.deserializer value.deserializer group.idImplementationtry (KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(properties)) { consumer.subscribe(List.of(TOPIC_NAME)); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(10)); records.forEach(KafkaConsumerApp::printRecord); }}Source of this section is available on GitHub.Kafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 7 - Kafka Challenges Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Part 7 - Kafka Challenges", "url": "/blog/posts/kafka-challenges/", "categories": "kafka", "tags": "kafka-series", "date": "2021-06-15 14:29:00 -0500", "snippet": "Confluent is one of the biggest Apache Kafka ecosystem contributors.And they have addressed many challenges that we might across during its uses.One of them is the Kafka Schema Registry.Kafka Schem...", "content": "Confluent is one of the biggest Apache Kafka ecosystem contributors.And they have addressed many challenges that we might across during its uses.One of them is the Kafka Schema Registry.Kafka Schema Registry provides serialization and deserialization schema support for Apache Avro Schema JSON Schema Protobuf Schema puts Avro serializers and deserializers as a first-class citizen provides support for schema registry and version management provides RESTful service-based discovery provides version compatibility reconciliationFor more information, please visit the site.Kafka ConnectKafka Connect is an api for the developers, intended to make the job of connectingdata sources and targets easier and more consistent. The goal is to standardized ona common approach for integrating diverse data sources with standard producer andconsumer applications.Currently there are more than 50 platform connectors available, that are designed toconnect to many different products and services, and its growing day by day.Confluent itself has created many of these connectors, for the full-list, you cancheck it out here.For more information, please visit the Apache Kafka Connectand Confluent Kafka Connect.Kafka StreamsIt is a new client library for real-time, stream-based processing. Any organization who are alreadyutilizing the kafka, can now have streaming capabilities without having to install, run and maintainother different platforms. It is the same as producer and consumer client library, by extending it withkafka streams library, we can utilize stream-based processing capabilities all within the same place.For more information, please visit the Apache Kafka Streamsand Confluent Kafka Streams.Kafka Series Part 1 - Kafka Key Terminology Part 2 - Kafka Partitions Part 3 - Run a single instance of Apache Kafka Part 4 - Run a multiple instances of Apache Kafka Part 5 - Kafka Producer Part 6 - Kafka Consumer Part 8 - Producer/Consumer using Kafka Client Part 9 - Producer/Consumer using Spring Kafka" }, { "title": "Precondition in Controller and Service", "url": "/blog/posts/precondition-in-controller-and-service/", "categories": "spring-validation,, validation", "tags": "spring-validation,, validation", "date": "2021-05-06 14:27:00 -0500", "snippet": "ProblemCurrently using Google’s Preconditions class to validate user’s input data.Problem 1:Validation check-in Controller only.@Controller...public void register(ProductInfo data) { Preconditio...", "content": "ProblemCurrently using Google’s Preconditions class to validate user’s input data.Problem 1:Validation check-in Controller only.@Controller...public void register(ProductInfo data) { Preconditions.checkArgument(StringUtils.hasText(data.getName()), \"Empty name parameter.\"); productService.register(data);}@Service...public void register(ProductInfo data) { productDao.register(data);}Problem 2:But there may be a chance that the register method in the Service layer would be used by another Controller method likebelow:@Controller...public void register(ProductInfo data) { productService.register(data);}public void anotherRegister(ProductInfo data) { productService.register(data);}@Service...public void register(ProductInfo data) { Preconditions.checkArgument(StringUtils.hasText(data.getName()), \"Empty name parameter.\"); productDao.register(data);}Question: Which is the better way of checking preconditions in controller or service?SolutionIdeally, it is better to do it in both places. Two different things need to be considered while validating in controllerand service: Validation (with error handling) Defensive Programming (aka assertions, precondition, aka design by contract).We should validate the controller and defensive programming in our service.Validation :We need to validate for forms and REST requests so that we can send a sensible error back to the client. This includeswhat fields are bad and then doing localization of the error messages… etc… In the above example, validation errorwould send a 500 error message with a stack trace if ProductInfo.name property is null which is not a good message forthe end-user.For this problem, Spring has a solutionfor validating objects in the controller.Defensive programming :It is done in the service layer but not validation because we don’t have access to locale to generate proper errormessages. Some people do but Spring doesn’t help us here.The other reason why validation is not done in the service layer is that the ORM already typically does this through theJSR Bean Validation spec (hibernate) but it doesn’t generate sensible error messages. example: constraint violation,data null, etc.One strategy people do is to create our own preconditions utils library that throws custom derived RuntimeExceptionsinstead of guava’s (and commons-lang) IllegalArgumentException and IllegalStateException and then try…catch theexceptions in the controller converting them to validation error messages." }, { "title": "Manage Multiple SSH Keys", "url": "/blog/posts/multiple-ssh-support/", "categories": "git", "tags": "git", "date": "2021-05-06 13:12:00 -0500", "snippet": "ScenarioI have multiple SSH keys, want to manage them without any trouble.I can do it in various ways: creating a directory for ssh key and switch every time I need to or even create a script for ...", "content": "ScenarioI have multiple SSH keys, want to manage them without any trouble.I can do it in various ways: creating a directory for ssh key and switch every time I need to or even create a script for that. use the same ssh key for all accounts like Github, Bitbucket, Gitlab, etc. But there is one catch, which isI cannot use the same ssh key on the same server like Github. Create a separate ssh key for all accounts and manage it using ssh config, which I am going to use in this post.Assumptions You have basic knowledge about Git and SSH. All your generated ssh keys will be inside the ~/.ssh directory.Step to create ssh config Generate ssh key for your account. The following command will generate private and public keys.ssh-keygen -t ed25519 -C \"&lt;EMAIL_ADDRESS&gt;\" Rename your ssh key and give it some meaningful name like in my case:mv ~/.ssh/id_ed25519 ~/.ssh/github_id_ed25519 Repeat 1 &amp; 2 Steps for all your accounts Create config file inside .ssh directory touch ~/.ssh/config Add the following configuration inside your config file. You can add/remove based on your needs: # GitHub Account 1 Host github.com HostName github.com User &lt;USER_NAME_1&gt; IdentityFile ~/.ssh/github_id_ed25519 # Github Account 2 Host github.com-&lt;USER_NAME_2&gt; HostName github.com User &lt;USER_NAME_2&gt; IdentityFile ~/.ssh/github2_id_ed25519 # Gitlab Account Host gitlab.com HostName gitlab.com User &lt;USER_NAME&gt; IdentityFile ~/.ssh/gitlab_id_ed25519 # Bitbucket Account Host bitbucket.org HostName bitbucket.org User &lt;USER_NAME&gt; IdentityFile ~/.ssh/bucket_id_ed25519 Note:For the Github account, we have defined two accounts. One with default HostName Another with extra content append to default HostName, which is USERNAME_2Also, note that I have added dash (-) after the HostName.How to use itIt’s simple. Just do what you were doing with a single SSH key for all accounts including GitHub Account 1.But for GitHub Account 2, we have to handle it differently.Let’s say if we have a URL like this git@github.com:&lt;USER_NAME_2&gt;/project.gitthen we need to append git@github.com -&lt;USER_NAME_2&gt; :/project.git.It's the only thing that we have to do before we use it.# For Github Account 2# Instead of thisgit clone git@github.com:&lt;USER_NAME_2&gt;/project.git# We have to use it in this waygit clone git@github.com-&lt;USER_NAME_2&gt;:&lt;USER_NAME_2&gt;/project.gitSome useful SSH command If your ssh-agent failed then you can restart it using the following commandeval $(ssh-agent -s) If you want to list all registered keysssh-add -l If you want to clear all registered keysssh-add -D" }, { "title": "Istio Access Metrics", "url": "/blog/posts/istio_architecture/", "categories": "kubernetes", "tags": "istio", "date": "2019-09-14 00:52:46 -0500", "snippet": "With Istio’s insight into how applications communicate, it can generate profound insights into how applications areworking and performance metrics.Generate LoadTo view the graphs, there first needs...", "content": "With Istio’s insight into how applications communicate, it can generate profound insights into how applications areworking and performance metrics.Generate LoadTo view the graphs, there first needs to be some traffic. Execute the command below to send requests to the application.while true; do curl -s https://product.cluster.com/productpage &gt; /dev/null echo -n .; sleep 0.2doneAccess DashboardsWith the application responding to traffic the graphs will start highlighting what’s happening under the covers.GrafanaThe first is the Istio Grafana Dashboard. The dashboardreturns the total number of requests currently being processed, along with the number of errors and the response time of each call.https://grafana.cluster.com/dashboard/db/istio-mesh-dashboardAs Istio is managing the entire service-to-service communicate, the dashboard will highlight the aggregated totals andthe breakdown on an individual service level.JaegerJaeger provides tracing information for each HTTP request. It shows which calls are made and where the time was spentwithin each request.https://jaeger.cluster.com/It is an excellent way to identify issues and potential performance bottlenecks.Service GraphAs a system grows, it can be hard to visualise the dependencies between services. The Service Graph willdraw a dependency tree of how the system connects.https://service-graph.cluster.com/dotviz" }, { "title": "Istio Archtecture", "url": "/blog/posts/istio_access_metrics/", "categories": "kubernetes", "tags": "istio", "date": "2019-09-14 00:52:46 -0500", "snippet": "Pilot Responsible for configuring the Envoy and Mixer at runtime.Proxy / Envoy Sidecar proxies per microservice to handle ingress/egress trafficbetween services in the cluster and from a service ...", "content": "Pilot Responsible for configuring the Envoy and Mixer at runtime.Proxy / Envoy Sidecar proxies per microservice to handle ingress/egress trafficbetween services in the cluster and from a service to external services.The proxies form a secure microservice mesh providing a rich set offunctions like discovery, rich layer-7 routing, circuit breakers,policy enforcement and telemetry recording/reporting functions.Mixer Create a portability layer on top of infrastructure backends.Enforce policies such as ACLs, rate limits, quotas, authentication,request tracing and telemetry collection at an infrastructure level.Citadel / Istio CA (Certificate Authority) Secures service to service communication over TLS. Providinga key management system to automate key and certificate generation,distribution, rotation, and revocation.Ingress/Egress Configure path based routing for inbound and outbound external traffic.Control Plane API Underlying Orchestrator such as Kubernetes or Hashicorp Nomad.ArchtectureReferenceenvoy A messenger or representative, especially one on a diplomatic mission.proxy The authority to represent someone else, especially in voting.ingress The action or fact of going in or entering.egress The action of going out of or leaving a place.telemetry The process of recording and transmitting the readings of an instrument.citadel A fortress, typically on high ground, protecting or dominating a city.in-bound Traveling toward a particular place, especially when returning to the original point of departure.out-bound Traveling away from a particular place, especially on the first leg of a round trip.orchestrate plan or coordinate the elements of (a situation) to produce a desired effect, especially surreptitiously.surreptitiously in a way that attempts to avoid notice or attention;" }, { "title": "Stream Processing", "url": "/blog/posts/stream_processing/", "categories": "technology", "tags": "middleware", "date": "2019-09-04 01:17:16 -0500", "snippet": "It isn’t enough to just read, write, and store streams of data,the purpose is to enable real-time processing of streams.In a stream processor is anything that takes continual streams of datafrom in...", "content": "It isn’t enough to just read, write, and store streams of data,the purpose is to enable real-time processing of streams.In a stream processor is anything that takes continual streams of datafrom input topics, performs some processing on this input, and producescontinual streams of data to output topics.For example, a retail application might take in input streams of sales andshipments, and output a stream of reorders and price adjustments computed off this data.It is possible to do simple processing directly using the producer and consumer APIs.Kafka Stream ProcessingFor more complex transformations Kafka provides a fully integrated Streams API.This allows building applications that do non-trivial processing that computeaggregations off of streams or join streams together.This facility helps solve the hard problems this type of application faces: handling out-of-order data reprocessing input as code changes performing stateful computations, etc.The streams API builds on the core primitives Kafka provides: it uses the producer and consumer APIs for input uses Kafka for stateful storage uses the same group mechanism for fault tolerance among the stream processor instances" }, { "title": "Spring Integration", "url": "/blog/posts/spring_integration/", "categories": "integration", "tags": "middleware", "date": "2019-08-26 06:31:02 -0500", "snippet": "IntroductionSpring Integration (SI) is a framework enabling a collection of individual applications tointegrate together to deliver a business enterprise system. The framework is essentially alight...", "content": "IntroductionSpring Integration (SI) is a framework enabling a collection of individual applications tointegrate together to deliver a business enterprise system. The framework is essentially alightweight messaging system that enables spring based applications to communicate with oneanother and supports integration with external systems via declarative adapters. It is basedon the ‘filters and pipes’ design architecture. A key feature of it is that it achieves thisintegration in a minimally intrusive way.Three main components:MessagesEncapsulate the data to be transferred from one place to another. They comprise of a headerand a payload (your data typically in the form of a POJO). Header (holds metadata) message id timestamp etc ChannelsProvide a mechanism to transport messages from one endpoint to another. Represents the pipesin the pipes &amp; filters architecture. SI offers two types of channels, namelyPollable and Subscribable Channels.The former rely on consumers to periodically check for messages whereasthe latter is directly responsible for notifying registered consumers when messages become available.EndpointsConsumer/Producer of messages. Performs some action based on the payload. Endpoints come invarious flavours, each performing a different function. Transformers (transform data) Routers (route data) Filters (filter data) Splitter (splits messages) Aggregator (aggregates group of messages into single message) Service Activator (connecting messages to Services) Channel Adapters (connect channels to external applications)Message ChannelA message channel is the component through which messages are moved so it can be thought as a pipebetween message producer and consumer. A Producer sends the message to a channel, and a consumerreceives the message from the channel. A Message Channel may follow either Point-to-Point or Publish/Subscribesemantics. With a Point-to-Point channel, at most one consumer can receive each message sent tothe channel. With Publish/Subscribe channels, multiple subscribers can receive each Message sentto the channel. Spring Integration supports both of these.Different channelsDirect channel and null-channel are used. Direct channel is the default channel type within SpringIntegration and simplest point-to-point channel option. Null Channel is a dummy message channel tobe used mainly for testing and debugging. It does not send the message from sender to receiverbut its send method always returns true and receive method returns null value. In addition toDirectChannel and NullChannel, Spring Integration provides different Message Channel Implementationssuch as PublishSubscribeChannel, QueueChannel, PriorityChannel, RendezvousChannel, ExecutorChannel and ScopedChannel.Message EndpointA message endpoint isolates application code from the infrastructure. In other words, it is anabstraction layer between the application code and the messaging framework.Main Message Endpoints :TransformerA Message Transformer is responsible for converting a Message’s content or structure and returningthe modified Message. For exampleit may be used to transform message payload from one format to another or to modify message header values.FilterA Message Filter determines whether the message should be passed to the message channel.RouterA Message Router decides what channel(s) should receive the Message next if it is available.SplitterA Splitter breaks an incoming message into multiple messages and send them to the appropriate channel.AggregatorAn Aggregator combines multiple messages into a single message.Service ActivatorA Service Activator is a generic endpoint for connecting a service instance to the messaging system.Channel AdapterA Channel Adapter is an endpoint that connects a Message Channel to external system. Channel Adaptersmay be either inbound or outbound.Inbound ChannelAn inbound Channel Adapter endpoint connects a external system to a MessageChannel.Outbound ChannelAn outbound Channel Adapter endpoint connects a MessageChannel to a external system.Messaging GatewayA gateway is an entry point for the messaging system and hides the messaging API from external system.It is bidirectional by covering request and reply channels.Also Spring Integration provides various Channel Adapters and Messaging Gateways (for AMQP, File, Redis,Gemfire, Http, Jdbc, JPA, JMS, RMI, Stream etc..) to support Message-based communication with externalsystems. Please visit Spring Integration Reference documentation for the detailed information." }, { "title": "Application Integration or Enterprise Application Integration", "url": "/blog/posts/application-integration/", "categories": "integration", "tags": "middleware", "date": "2019-08-26 06:31:02 -0500", "snippet": "It’s the sharing of processes and data among different applications in an enterprise.Different levels of integration Presentation Level Business Process Level Data Level Communications LevelPre...", "content": "It’s the sharing of processes and data among different applications in an enterprise.Different levels of integration Presentation Level Business Process Level Data Level Communications LevelPresentation LevelIn this level, Integration is achieved by presenting several different applicationsas a single application with a common user inteface (UI). Also known as Screen Scraping.This older approach to integration involves using : middleware technology to collect theinformation that a user enters into a web page or some other user interface.Business Process LevelThe logical processes required by an enterprise to conduct its business are mapped onto itsIT assets, which often reside in different parts of the enterprise and increasingly, the cloud.By identifying individual actions in a workflow and approaching their IT assets as a meta-system(i.e. a system of systems), enterprises can use applications integration to define how individualapplications will interact in order to automate crucial business processes, resulting in the fasterdelivery of goods and services to customers, reduced chances for human error, and loweroperational costs.Data LevelAside from business process integration, data integration is also required for successfulapplications integration. If an application can’t exchange and understand data from anotherapplication, inconsistencies can arise and business processes become less efficient.Data integration is achieved by either writing code that enables each application to understanddata from other applications in the enterprise or by making use of an intermediate data formatthat can be interpreted by both sender and receiver applications. The latter approach is preferableover the former since it scales better as enterprise systems grow in size and complexity.In both cases, access, interpretation, and data transformation are important capabilities forsuccessfully integrating data.Communications LevelUnderlying business process and data integration are communications-level integration. This refersto how different applications within an enterprise talk to each other, either through file transfer,request/reply methods, or messaging. In many cases, applications weren’t designed to communicate witheach other, requiring technologies for enabling such communication. These includeApplication Programming Interfaces (APIs), which specify how applications can be called, and connectorsthat act as intermediaries between applications. At the communications level, it is also important toconsider the architecture of interactions between applications, which can be integrated according toa point-to-point model, hub-and-spoke approach, or with an Enterprise Service Bus (ESB).Synchronous vs. Asynchronous CommunicationWithout effective communication, business processes and data cannot be properly integrated. Depending onan enterprise’s particular needs, communication can be either synchronous, asynchronous,or some combination of the two.In synchronous communication, a sender application sends a request to a receiver application and must waitfor a reply before it can continue with its processing. This pattern is typically used in scenarios where datarequests need to be coordinated in a sequential manner.In asynchronous communication, a sender application sends a message to a receiver application and continuesits processing before receiving a response. In other words, the sender application does not depend onthe receiver application to complete its processing. If multiple applications are being integrated in sucha fashion, the sender application can complete its processing even if other subprocesses have notfinished processing." }, { "title": "Data Pipelines", "url": "/blog/posts/data_pipeline/", "categories": "technology", "tags": "middleware", "date": "2019-08-10 13:40:48 -0500", "snippet": "Concern regarding building a data processing system multiple layered system volume of data result are not needed to be sent back to client immediately event processing pipelineMain Message Bus ...", "content": "Concern regarding building a data processing system multiple layered system volume of data result are not needed to be sent back to client immediately event processing pipelineMain Message Bus (Kafka) Scaling Message semantics Repeated pattern of Reading and Writing to the bus makes a pipeline of event processing.Prerequisite question regarding Data Intensive System Is the data complete or meaning of it? Is it consistent with other data sources, in case of data lake or olap? If it is reprocessed, is that data reflective of that, are the SLA's (Service-level agreement) all met? If some downstream service was down for sometime, did that impact the final data?Concern with pipelines How stable are the underlying systems which generate the data? How reliable is the final data even when working with components which go down?1. Downtime of service Services could/should/do go down What should a consumer of a service do under such a situation? What if, it is a component of a data pipelineSome solutions Fail fast Implement a Dead Letter Queue The service takes responsibility of ensuring the message is processed2. Slow consumers silent problem in data pipelines when consumer receives more than it can handle hard to keep track until data loss happens leads to cascading failures of all systems when there is a change in contract or schema.Some solutions Measuring and scaling consumer Contract and schema evolution3. Ensuring zero data loss How to make it reliable? How do you build a system which continuously processes data with only a few hours of buffer for messages?4. Global view of the schema setting the blueprint/data model of data like type, default value, placement etc need something to agreed upon by all the producer and consumerContract A good contract should look like model classes in code-base With interactions between different model classes are driven more by the responsibilityContract RelationshipMessage FormatThriftThrift is an interface definition language and binary communication protocol used for defining and creating services fornumerous languages. It forms a remote procedure call framework.Advantages of Thrift String typing of fields Easy renaming of fields and types Bindings for all major languages Very fast SerDe for messagesThrift ContractCompatibilityForward Compatibility and Backward Compatibility Producers and Consumers work on different versions of the contractbut are able to work well. Forward Compatibility Ensure data written today is able to be read and processed in future. Backward Compatibility Ensure past data read today is able to be read and processed. Full Compatibility Consumers with version X of schema are able to read datagenerated by X, X-1 but not necessarily X-2. Data generated by producers with version X of schema is ableto be processed by consumers running X, X-1 version of schema,not necessarily X-2. While evolving schema in a compatible way, needs to consider transitivity. All the data written in the past may not be Fully Compatible in future.Full Transitive Compatibility via Thrift Ensure that the required fields are never changed/removed New fields added are only optional The index tags are never changed for a field Do not remove optional fields, rename them to deprecatedReferences Schemas Contract Compatibility Thrift Avro Schema Evolution Schema Registry Spring Schema Evolution Schema Evolution Apache Samza Apache Flume Apache MapReduce Service Level Agreement (SLA) Wikipedia Definition Data Lake" }, { "title": "Generic Enum Converter for JPA", "url": "/blog/posts/generic_enum_converter_for_jpa/", "categories": "technology", "tags": "jpa", "date": "2019-01-27 13:04:46 -0600", "snippet": "DescriptionIn following example, we have define QualityMeasureType enum and used in class using @Enumerated annotation.Here, JPA will convert enum value into string and store it in db and vice-vers...", "content": "DescriptionIn following example, we have define QualityMeasureType enum and used in class using @Enumerated annotation.Here, JPA will convert enum value into string and store it in db and vice-versa.enum QualityMeasureType {\tA, B;}@Enumerated(EnumType.STRING)private QualityMeasureType type;So, the problem here is if we try to store additional value in db, shown in below example.@Enumerated annotation will not work. Because it will try to store PUBLIC in db. If there is already‘P’ value in db then it cannot convert it to PUBLIC.To support this, we need to add some converter which converts this data into enum and enum into data.enum SavedQmListScope {\tPUBLIC(\"P\"),\tPRIVATE(\"M\");}@Enumerated(EnumType.STRING)private SavedQmListScope scope;To resolve this issue, there are many ways to do it: By using javax.persistence.AttributeConverter interface Define a custom annotation, patch the JPA provider to recognize this annotationProblem:How to solve above issue for enum which have additional value using JPA/Hibernate?How to make it workable for all type of enum?Solution 1:We can create a custom converter implementing AttibuteConverter. And then inject it into the property field.But there may be a chance that every enum will be having their own converter.So in-order to reduce the process of writing an enum converter class to pure boilerplate. The components of thissolution are: PersistableEnum Interface:The contract for an enum class that grants access to a String value for each enum constant (also a factory for gettingthe enum constant for a matching value) AbstractEnumConverter Class: Provides the common code for translating values to/from enum constants Java Enum Classes: That implement the PersistableEnum interface JPA Converter Classes: That extend the AbstractEnumConverter classThe PersistableEnum interface is simple and contains a static factory method, forValue(), for obtaining enum constants:public interface PersistableEnum { String getValue(); public static &lt;E extends Enum&lt;E&gt; &amp; PersistableEnum&gt; E forValue(Class&lt;E&gt; cls, String tok) { final String t = tok.trim().toUpperCase(); return Stream.of(cls.getEnumConstants()) .filter(e -&gt; e.getValue().equals(t)) .findFirst() .orElseThrow(() -&gt; new IllegalArgumentException(\"Unknown value '\" + tok + \"' for enum \" + cls.getName())); }}The AbstractEnumConverter class is a generic class that is also simple. It implements the JPA 2.1 AttributeConverterinterface but provides no implementations for its methods (because this class can’t know the concrete types needed forobtaining the appropriate enum constants). Instead, it defines helper methods that the concrete converter classes willchain to:public abstract class AbstractEnumConverter&lt;E extends Enum&lt;E&gt; &amp; PersistableEnum&gt; implements AttributeConverter&lt;E, String&gt; { public String toDatabaseColumn(E attr) { return (attr == null) ? null : attr.getValue(); } public E toEntityAttribute(Class&lt;E&gt; cls, String dbCol) { return (dbCol == null) ? null : PersistableEnum.forValue(cls, dbCol); }}An example of a concrete enum class that could now be persisted to a database with the JPA 2.1 Converter facility isshown below (note that it implements PersistableEnum, and that the value for each enum constant is defined as a privatefield):public enum SavedQmListScope implements PersistableEnum { PUBLIC(\"P\"),\tPRIVATE(\"M\"); final String e_value; SavedQmListScope(String v) { this.e_value = v; } @Override public String getValue() { return this.e_value; }}The boilerplate for every JPA 2.1 Converter class would now look like this (note that every such converter will need toextend AbstractEnumConverter and provide implementations for the methods of the JPA AttributeConverter interface):@Converterpublic class SavedQmListScopeConverter extends AbstractEnumConverter&lt;SavedQmListScope&gt; { @Override public String convertToDatabaseColumn(SavedQmListScope attribute) { return this.toDatabaseColumn(attribute); } @Override public SavedQmListScope convertToEntityAttribute(String dbData) { return this.toEntityAttribute(SavedQmListScope.class, dbData); }}Solution 2:By extending Solution 1, we can enhance its funtionality by making it more generic:Interface enum must implement:public interface PersistableEnum&lt;T&gt; { public T getValue();}Base abstract converter:@Converterpublic abstract class AbstractEnumConverter&lt;T extends Enum&lt;T&gt; &amp; PersistableEnum&lt;E&gt;, E&gt; implements AttributeConverter&lt;T, E&gt; { private final Class&lt;T&gt; clazz; public AbstractEnumConverter(Class&lt;T&gt; clazz) { this.clazz = clazz; } @Override public E convertToDatabaseColumn(T attribute) { return attribute != null ? attribute.getValue() : null; } @Override public T convertToEntityAttribute(E dbData) { T[] enums = clazz.getEnumConstants(); for (T e : enums) { if (e.getValue().equals(dbData)) { return e; } } throw new UnsupportedOperationException(); }}You must create a converter class for each enum, I find it easier to create static class inside the enum:public enum SavedQmListScope implements PersistableEnum&lt;String&gt; {\tPUBLIC(\"P\"),\tPRIVATE(\"M\"); private final String value; @Override public String getValue() { return value; } private SavedQmListScope(String value) { this.value= value; } public static class Converter extends AbstractEnumConverter&lt;SavedQmListScope, String&gt; { public Converter() { super(SavedQmListScope.class); } }}And mapping example with annotation:class SavedQmList {\t@Convert(converter = SavedQmListScope.Converter.class)\tprivate SavedQmListScope SavedQmListScope;}" } ]
